{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Play with dataset & few-shot learning"
      ],
      "metadata": {
        "id": "Lc5E0_xg3YCg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92yYRwvrmwV6"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --disable-pip-version-check \\\n",
        "torch==1.13.1 \\\n",
        "torchdata==0.5.1 --quiet\n",
        "\n",
        "! pip install \\\n",
        "transformers==4.27.2 \\\n",
        "datasets==2.11.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_datasets\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "6bWyvj3zruUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hugging_face_dataset_name = 'knkarthick/dialogsum'\n",
        "dataset = load_datasets(hugging_face_dataset_name)"
      ],
      "metadata": {
        "id": "GgbIBaTgsMZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [40, 100]\n",
        "dash_line = \"-\"*150\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print(\"example \", i+1)\n",
        "    print(dash_line)\n",
        "    print(\"Input Dialog\")\n",
        "    print(dataset[\"test\"][index][\"dialogue\"])\n",
        "    print(dash_line)\n",
        "    print(\"Baseline Human summary\")\n",
        "    print(dataset[\"test\"][index][\"summary\"])\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zP3zbieRsqL8",
        "outputId": "d661e7c1-afc5-4175-d6e4-efb1cef9ab98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'------------------------------------------------------------------------------------------------------------------------------------------------------'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "model = AutoModelForSeq2SeqLM.frompretrained(model_name)\n",
        "tokenizer = AutoTokenizer.frompretrained(model_name, use_fast = True)"
      ],
      "metadata": {
        "id": "jslPvYMJssZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## check tokenizer\n",
        "sentence = \"what time is it, tom?\"\n",
        "sentence_encoded = tokenizer(sentence, return_tensor = 'pt')\n",
        "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0], skip_special_tokens = True)\n",
        "\n",
        "print(\"encoded sent\")\n",
        "print(sentence_encoded['input_ids'][0])\n",
        "\n",
        "print(\"\\ndecoded sent\")\n",
        "print(sentence_decoded)"
      ],
      "metadata": {
        "id": "nuavRnz35Pw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialog = dataset[\"test\"][index][\"dialogue\"]\n",
        "    summary = dataset[\"test\"][index][\"summary\"]\n",
        "    inputs = tokenizer(dialog, return_tensor = 'pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(inputs['input_ids'],\n",
        "                       max_new_tokens = 50)[0],\n",
        "    skip_special_tokens = True)\n",
        "    print(dash_line)\n",
        "    print(\"example \", i+1)\n",
        "    print(dash_line)\n",
        "    print(\"Input prompt\")\n",
        "    print(dialog)\n",
        "    print(dash_line)\n",
        "    print(\"Baseline Human summary\")\n",
        "    print(summary)\n",
        "    print(dash_line)\n",
        "    print(\"Model generation without prompt: \", output)\n",
        "    print()"
      ],
      "metadata": {
        "id": "484pYLMp6E49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialog = dataset[\"test\"][index][\"dialogue\"]\n",
        "    summary = dataset[\"test\"][index][\"summary\"]\n",
        "    prompt = f\"\"\"\n",
        "    Summarize the following conversation.\n",
        "    {dialog}\n",
        "\n",
        "    Summary:\n",
        "    \"\"\"\n",
        "    # prompt = f\"\"\"\n",
        "    # Dialogue:\n",
        "    # {dialog}\n",
        "\n",
        "    # What was going on?:\n",
        "    # \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensor = 'pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(inputs['input_ids'],\n",
        "                       max_new_tokens = 50)[0],\n",
        "    skip_special_tokens = True)\n",
        "    print(dash_line)\n",
        "    print(\"example \", i+1)\n",
        "    print(dash_line)\n",
        "    print(\"Input prompt\")\n",
        "    print(dialog)\n",
        "    print(dash_line)\n",
        "    print(\"Baseline Human summary\")\n",
        "    print(summary)\n",
        "    print(dash_line)\n",
        "    print(\"Model generation without prompt: \", output)\n",
        "    print()"
      ],
      "metadata": {
        "id": "MfVA7pqvBpB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        dialog = dataset[\"test\"][index][\"dialogue\"]\n",
        "        summary = dataset[\"test\"][index][\"summary\"]\n",
        "\n",
        "        prompt += f\"\"\"\n",
        "        Dialogue:\n",
        "\n",
        "        {dialog}\n",
        "\n",
        "        What was going on?\n",
        "        {summary}\n",
        "        \"\"\"\n",
        "\n",
        "        dialog = dataset[\"test\"][example_indices_full][\"dialogue\"]\n",
        "        prompt += f\"\"\"\n",
        "        Dialogue:\n",
        "\n",
        "        {dialog}\n",
        "\n",
        "        What was going on?\n",
        "        \"\"\"\n",
        "        return prompt\n"
      ],
      "metadata": {
        "id": "xrV88e1EZJs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40]\n",
        "example_indices_to_summarize = 200\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_indices_to_summarize)\n",
        "one_shot_prompt"
      ],
      "metadata": {
        "id": "QffF9kl5Y2Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation_config = GenerationConfig(max_new_tokens = 50, do_sample = True, temperature = 0.2)\n",
        "summary = dataset['test'][example_indices_to_summarize]['summary']\n",
        "input = tokenizer(one_shot_prompt, return_tensor = 'pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(inputs['input_ids'],\n",
        "                       max_new_tokens = 50)[0],\n",
        "    # generation_config = generation_config,\n",
        "    skip_special_tokens = True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f\"baseline human summary \\n{summary}\")\n",
        "print(dash_line)\n",
        "print(f\"Model generation one shot \\n {output}\")"
      ],
      "metadata": {
        "id": "Age3-mtrvam_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "Rek4QOCG3bjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --disable-pip-version-check \\\n",
        "torch==1.13.1 \\\n",
        "torchdata==0.5.1 --quiet\n",
        "\n",
        "! pip install \\\n",
        "transformers==4.27.2 \\\n",
        "datasets==2.11.0\\\n",
        "evaluate==0.4.0 \\\n",
        "rouge_score==0.1.1 \\\n",
        "peft==0.3.0 --quiet"
      ],
      "metadata": {
        "id": "AidDVl4o3dtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_datasets\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "AB9A2Ufy46iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hugging_face_dataset_name = 'knkarthick/dialogsum'\n",
        "dataset = load_datasets(hugging_face_dataset_name)\n",
        "dadaset"
      ],
      "metadata": {
        "id": "C_BJgj9t67oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "original_model = AutoModelForSeq2SeqLM.frompretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)"
      ],
      "metadata": {
        "id": "JVf8I77p7Lpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_trainable_param(model):\n",
        "    trainable_param = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.require_grad:\n",
        "            trainable_param += param.numel()\n",
        "    return trainable_param, all_param\n",
        "\n",
        "no_trainable_param(original_model)"
      ],
      "metadata": {
        "id": "sUXaqeQC42Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = \"Summarize the following convo \\n\\n\"\n",
        "    end_prompt = \"\\n\\nSummary\"\n",
        "    prompt = [start_prompt+dialogue+end_prompt for dialogue in example['dialogue']]\n",
        "    example['input_ids'] = tokenizer(prompt, padding='max_length', truncation = True, return_tensors = 'pt').input_ids\n",
        "    example['labels'] = tokenizer(example['summary'], padding='max_length', truncation = True, return_tensors = 'pt').input_ids\n",
        "    return example\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batch = True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'topic', 'dialog', 'summary'])\n",
        "tokenized_dataset = tokenized_dataset.filter(lambda example, index: index%100==0, with_indices = True)"
      ],
      "metadata": {
        "id": "Z_5teGV353tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = f\"./dialog_summary_traing_{str(int(time.time()))}\"\n",
        "train_args = TrainingArguments(\n",
        "    output_dir = out_dir,\n",
        "    learning_rate = 1e-5,\n",
        "    num_train_epochs = 1,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 1,\n",
        "    max_steps = 1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = original_model,\n",
        "    args = train_args,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        "    eval_dataset = tokenized_dataset['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "mIM3QbZ49TOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Kn4o1OJW-dNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxoTSGrE-dAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE\n",
        "rouge = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "ppjTjGwEAkQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogs = dataset[\"test\"][0:10][\"dialogue\"]\n",
        "summary = dataset[\"test\"][0:10][\"summary\"]\n",
        "original_model_summary = []\n",
        "for _, dialog in enumerate(dialogs):\n",
        "    prompt = f\"\"\"\n",
        "    Summarize the following convo:\n",
        "    {dialog}\n",
        "\n",
        "    Summary:\"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensor = 'pt').input_ids\n",
        "    original_model_outputs = original_model.generate(input_ids = input_ids, generation_config = GenerationConfig(max_new_tokens = 200))\n",
        "    original_model_text_outputs = tokenizer.decode(original_model_outputs[0], skip_special_tokens = True)\n",
        "    original_model_summary.append(original_model_text_outputs)"
      ],
      "metadata": {
        "id": "JgymRLNZBYtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_results = rouge.compute(\n",
        "    prediction = original_model_summaries,\n",
        "    reference = human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator = True,\n",
        "    use_stemmer = True\n",
        ")\n",
        "original_model_results"
      ],
      "metadata": {
        "id": "yE-ab3Vnx3NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PEFT"
      ],
      "metadata": {
        "id": "P9HbqEtG5gFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r =32, # Rank\n",
        "    lora_alpha = 32,\n",
        "    target_modules = ['q', 'v'],\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.seq_2_seq_LM # Flan-T5\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(original_model, lora_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ghr4iPeT4dej",
        "outputId": "2c514644-e08d-4ac3-e2cd-dfed5faa94ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(9, 1), (8, 2), (7, 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = f\"./peft_dialog_summary_traing_{str(int(time.time()))}\"\n",
        "peft_train_args = TrainingArguments(\n",
        "    output_dir = out_dir,\n",
        "    learning_rate = 1e-3,\n",
        "    auto_find_batch_size = True,\n",
        "    num_train_epochs = 1,\n",
        "    logging_steps = 1,\n",
        "    max_steps = 1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model = peft_model,\n",
        "    args = peft_train_args,\n",
        "    train_dataset = tokenized_dataset['train'],\n",
        ")"
      ],
      "metadata": {
        "id": "vZ_fkymP4zls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "peft_model_path = f\"./peft-dialog-summary-checkpoint-local\"\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "ppIq0It77mdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging peft adapter with orignal LLM\n",
        "from peft import PeftModel, PeftConfig\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype = torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       f\"./peft-dialog-summary-checkpoint-local\",\n",
        "                                       torch_dtype = torch.bfloat16,\n",
        "                                       is_trainable = False)"
      ],
      "metadata": {
        "id": "NEanYKyA8_FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferences"
      ],
      "metadata": {
        "id": "pcK3AF8HC-fL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}