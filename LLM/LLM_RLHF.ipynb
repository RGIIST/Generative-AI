{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i65c-ASvDtZe"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --disable-pip-version-check \\\n",
        "torch==1.13.1 \\\n",
        "torchdata==0.5.1 --quiet\n",
        "\n",
        "! pip install \\\n",
        "transformers==4.27.2 \\\n",
        "datasets==2.11.0\\\n",
        "evaluate==0.4.0 \\\n",
        "rouge_score==0.1.1 \\\n",
        "peft==0.3.0\n",
        "trl==0.4.4 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, AutoModelForSequenceclassification\n",
        "from datasets import load_datasets\n",
        "import torch\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from peft import LoraConfig, PeftModel, PeftConfig, TaskType\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "uQGWm698EJ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "hugging_face_dataset_name = 'knkarthick/dialogsum'\n",
        "dataset_original = load_datasets(hugging_face_dataset_name)\n",
        "dataset_original"
      ],
      "metadata": {
        "id": "aFmAAQgSLIyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(\n",
        "    model_name,\n",
        "    dataset_name,\n",
        "    input_min_text_length,\n",
        "    input_max_text_length):\n",
        "\n",
        "    dataset = load_datasets(dataset_name, split = 'train')\n",
        "    # dataset = dataset.filter(lambda x: len(x['dialogue'] > input_min_text_length and len(x['dialogue'] <= input_max_text_length, ))\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map = 'auto')\n",
        "    def tokenize(sample):\n",
        "        prompt = f\"\"\"\n",
        "        Summarize the following convo:\n",
        "        {sample['dialogue']}\n",
        "\n",
        "        Summary:\"\"\"\n",
        "\n",
        "        sample['input_ids'] = tokenizer.encode(prompt)\n",
        "        sample['query'] = tokenizer.decode(sample['input_ids'])\n",
        "        return sample\n",
        "\n",
        "    dataset = dataset.map(tokenize, batched = False)\n",
        "    dataset.set_format(type = 'torch')\n",
        "    dataset_splits = dataset.train_test_split(test_size = 0.2, shuffle = False, seed =42)\n",
        "    return dataset_splits"
      ],
      "metadata": {
        "id": "DK3fJqVDLjqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = build_dataset(model_name = model_name,\n",
        "                        dataset_name = hugging_face_dataset_name,\n",
        "                        input_min_text_length=200,\n",
        "                        input_max_text_length=1000)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "Ub0z65g9LuNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r =32, # Rank\n",
        "    lora_alpha = 32,\n",
        "    target_modules = ['q', 'v'],\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.seq_2_seq_LM # Flan-T5\n",
        ")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.frompretrained(model_name,\n",
        "                                             torch_dtype = torch.bfloat16)\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(model,\n",
        "                                       f\"./peft-dialog-summary-checkpoint-local\",\n",
        "                                       torch_dtype = torch.bfloat16,\n",
        "                                       device_map = 'auto',\n",
        "                                       is_trainable = True)\n",
        "\n",
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
        "                                                               torch_dtype = torch.bfloat16,\n",
        "                                                               is_trainable = True)\n",
        "\n",
        "ref_model = create_reference_model(ppo_model)\n",
        "\n",
        "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "toxicity_tokenizer = tokenizer.from_pretrained(toxicity_model_name, device_map = 'auto')\n",
        "toxicity_model = AutoModelForSequenceclassification.from_pretrained(toxicity_model_name, device_map = 'auto')"
      ],
      "metadata": {
        "id": "kn4QGK77O2qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline"
      ],
      "metadata": {
        "id": "o-aR-JVwMvXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is _available() else \"cpu\"\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                            model toxicity_model_name,\n",
        "                            device device)\n",
        "\n",
        "reward_logits_kwargs = {\n",
        "\"top_k\": None, # Return all scores.\n",
        "\"function to apply\": \"none\", # Set to \"none\" to retrieve raw logits\n",
        "\"batch size\": 16\n",
        "}\n",
        "reward_probabilities_kwargs = {\n",
        "\"top_k\": None, # Return all scores.\n",
        "\"function_ to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities\n",
        "\"batch size\": 16\n",
        "}\n",
        "print (\"Reward model output for non-toxic text:\")\n",
        "print (sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
        "print (sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
        "print(\"InReward model output for toxic text:\")\n",
        "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment pipe(toxic _text, **reward probabilities kwargs))"
      ],
      "metadata": {
        "id": "F_6pFsayMwth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1.41e-5\n",
        "max_ppo_epochs = 1\n",
        "mini_batch_size = 4\n",
        "batch_size = 16\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name = model_name,\n",
        "    learning_rate = learning_rate,\n",
        "    ppo_epochs =  max_ppo_epochs,\n",
        "    mini_batch_size = mini_batch_size,\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "def collator (data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config = config,\n",
        "    model = ppo_model,\n",
        "    ref_model = ref_model,\n",
        "    tokenizer = tokenizer,\n",
        "    dataset = v['train'],\n",
        "    data_collator = collator\n",
        ")"
      ],
      "metadata": {
        "id": "2qwKKYGVk8PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_min_length = 100\n",
        "output_max_length = 400\n",
        "output_length_sampler = LengthSampler(output _min_length, output_max_length)\n",
        "\n",
        "generation_kwargs = {\n",
        "\"min_length\": 5,\n",
        "\"top_k\": 0.0,\n",
        "\"top_p\": 1.0,\n",
        "\"do_sample\": True\n",
        "}\n",
        "\n",
        "reward kwargs = {\n",
        "\"top_k\": None, # Return all scores.\n",
        "\"function to apply\": \"none\", # You want the raw logits without softmax\n",
        "\"batch_size\": 16\n",
        "}\n",
        "\n",
        "max_ppo_steps = 10\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Break when you reach max steps.\n",
        "    if step >= max_ppo _steps:\n",
        "        break\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "    # Get response from FLAN-T5/PEFT LLM.\n",
        "    summary_tensors = []\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        max_new_tokens = output_length_sampler()\n",
        "        generation_kwargs [\"max new tokens\"] = max new tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation kwargs)\n",
        "\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "        # This needs to be called \"response\"\n",
        "        batch[\"response\"]= [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "        # Compute reward outputs.\n",
        "        query_response _pairs = [q+ r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "        rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "        # You use the nothate' item because this is the score for the positive nothate' class\n",
        "        reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "        # Run PPO step.\n",
        "        stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "        ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "        print(f'objective/kl: {stats [\"objective/kl\"]}')\n",
        "        print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "        print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages mean\"]}')\n",
        "        print('-'.join('' for x in range (100)))"
      ],
      "metadata": {
        "id": "Icy67dW3mjSN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}